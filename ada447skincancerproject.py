# -*- coding: utf-8 -*-
"""ada447skinCancerproject.ipynb adlı not defterinin kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18F8e-DWC7tcpDTXk0kMKljacWwxh2yo_
"""

!pip install -U fastai
!pip install -U kaggle
!pip install -U matplotlib

from google.colab import files
files.upload()  # Burada az önce indirdiğin `kaggle.json` dosyasını yükle

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000
!unzip -q skin-cancer-mnist-ham10000.zip -d skin_data/

!unzip -q /content/skin-cancer-mnist-ham10000.zip -d /content/skin_data/

from pathlib import Path

# Verinin bulunduğu ana dizini belirtelim
data_path = Path("/content/skin_data")

# Dizin yapısını inceleyelim
print("📁 Ana dizin içeriği:")
for item in data_path.iterdir():
    print(item)

# Alt klasörlerden bazılarını kontrol edelim (örn: HAM10000_images_part_1)
images_part1 = data_path / "HAM10000_images_part_1"
images_part2 = data_path / "HAM10000_images_part_2"

print("\n📷 İlk 5 Görsel - Part 1:")
print(list(images_part1.glob("*.jpg"))[:5])

print("\n📷 İlk 5 Görsel - Part 2:")
print(list(images_part2.glob("*.jpg"))[:5])

""" Tüm dosyalar sorunsuz yüklenmiş Şimdi CSV’den etiket bilgilerini çekip veri çerçevesini hazırlayacağım:"""

import pandas as pd

# Metadata dosyasını yükleyelim
meta_path = data_path / "HAM10000_metadata.csv"
metadata = pd.read_csv(meta_path)

print("📄 Metadata İlk 5 Satır:")
print(metadata.head())

print("\n🧾 Eşsiz Tanı (lesion types):")
print(metadata["dx"].unique())

print("\n🔢 Toplam Görsel Sayısı:", len(metadata))

"""Mükemmel, veri eksiksiz! Etiketler şu an kısaltma, bunların tam anlamlarını da ekleyelim ki rapor ve analizlerde açıklayıcı olsun."""

# Etiket Açıklamaları
label_map = {
    'nv': 'Melanocytic Nevi (Benign)',
    'mel': 'Melanoma (Malignant)',
    'bkl': 'Benign Keratosis-like Lesions (Benign)',
    'bcc': 'Basal Cell Carcinoma (Malignant)',
    'akiec': 'Actinic Keratoses and Intraepithelial Carcinoma (Malignant)',
    'vasc': 'Vascular Lesions (Benign)',
    'df': 'Dermatofibroma (Benign)'
}

# Etiketleri açıklamalı hale getirelim
metadata['dx_full'] = metadata['dx'].map(label_map)

print(metadata[['dx', 'dx_full']].drop_duplicates())

"""test ve veri setini ayırmak 80 e 20"""

from sklearn.model_selection import train_test_split

# Etiketli DataFrame'in tam dosya yollarını da ekleyelim
metadata['image_path'] = metadata['image_id'].apply(
    lambda x: f"/content/skin_data/HAM10000_images_part_1/{x}.jpg"
    if Path(f"/content/skin_data/HAM10000_images_part_1/{x}.jpg").exists()
    else f"/content/skin_data/HAM10000_images_part_2/{x}.jpg"
)

# Eğitim ve test setlerini ayırıyoruz (Stratify ile dengeli dağılım sağlıyoruz)
train_df, test_df = train_test_split(metadata, test_size=0.2, stratify=metadata['dx'], random_state=42)

print(f"🟢 Eğitim Seti: {len(train_df)} görsel")
print(f"🔵 Test Seti: {len(test_df)} görsel")

"""DataBlock ve DataLoader Oluşturma"""

from fastai.vision.all import *

# Sınıf etiketlerini belirleyelim
classes = metadata['dx'].unique().tolist()

# DataBlock tanımı
skin_lesion_block = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_x=ColReader('image_path'),
    get_y=ColReader('dx'),
    splitter=IndexSplitter(test_df.index),
    item_tfms=Resize(224),  # Presizing
    batch_tfms=aug_transforms(size=224)  # Data Augmentation
)

# DataLoader oluşturma
dls = skin_lesion_block.dataloaders(metadata, bs=32)

# Eğitim setinden örnekler görelim
dls.show_batch(max_n=9, figsize=(8,8))

learn = vision_learner(dls, resnet34, metrics=[accuracy, RocAuc()], pretrained=True)
learn.fine_tune(5)  # 5 Epoch ile başlayalım, ardından LR Finder ile ayar yapacağız

"""ilk basit eğitimin sonuçları"""

from fastai.vision.all import *
from pathlib import Path
import pandas as pd

# Ana dizin
path = Path('/content/skin_data')

# Metadata dosyasını yükleyelim
metadata = pd.read_csv(path/'HAM10000_metadata.csv')

# Doğru görsel yollarını belirtelim
def correct_image_path(image_id, base_path):
    # Construct potential relative paths
    relative_path1 = Path('HAM10000_images_part_1')/f'{image_id}.jpg'
    relative_path2 = Path('HAM10000_images_part_2')/f'{image_id}.jpg'

    # Check if the file exists using the base path and the relative path
    if (base_path/relative_path1).exists():
        return relative_path1
    elif (base_path/relative_path2).exists():
        return relative_path2
    else:
        # Return None if the file is not found in either location
        return None

# DataFrame'e tam görsel yollarını ekleyelim
# Pass the base_path to the correct_image_path function and store the relative path
metadata['image_path'] = metadata['image_id'].apply(lambda x: correct_image_path(x, path))

# DataLoader için DataFrame hazırlığı
# Sadece gerekli kolonları seçerek ve bir kopyasını oluşturarak olası sorunları azaltalım
df = metadata[['image_path', 'dx']].copy()

# Kolon ismini değiştirelim
df.rename(columns={'dx': 'label'}, inplace=True)

# Eksik yolları olan satırları temizle (correct_image_path None döndürdüğünde)
df = df.dropna(subset=['image_path'])

# Final DataFrame
print(f"✅ Kalan görsel sayısı: {len(df)}")

# DataLoader oluşturma
# Explicitly pass the path argument to ImageDataLoaders.from_df
dls = ImageDataLoaders.from_df(
    df,
    path=path, # Ensure fastai knows the base path for the DataFrame
    valid_pct=0.2,
    seed=42,
    fn_col='image_path', # This now contains relative paths
    label_col='label',
    item_tfms=Resize(224),
    bs=64
)

dls.show_batch(max_n=9, figsize=(8,8))

"""Model Kurulumu ve Eğitimi (A.4.1 Benchmark Model)

Burada learning rate kullanıldı ama bunu sabit tutup 1e-3 şeklinde kullandık. Optimize edilmedi. İleriki modellerde optimize edilip en iyi sayı bulunacak.
"""

from fastai.metrics import RocAuc

learn = vision_learner(dls, resnet34, metrics=[accuracy, RocAuc()], pretrained=True)

# Learning Rate Finder (opsiyonel)
learn.lr_find()

# Model Eğitimi
learn.fine_tune(5, base_lr=1e-3)

"""| Özellik            | İlk Model (Benchmark) | İkinci Model (Advanced) |
| ------------------ | --------------------- | ----------------------- |
| Başlangıç Accuracy | %73.5 (0.735)         | %76.6 (0.766)           |
| Son Accuracy       | %85.1 (0.851)         | %83.7 (0.837)           |
| ROC-AUC Başlangıç  | 0.88                  | 0.90                    |
| ROC-AUC Final      | 0.97                  | 0.95                    |
| Valid Loss (Final) | 0.417                 | 0.550                   |
| Epoch Sayısı       | 5                     | 5                       |

Farklılıkların Sebebi Nedir?

| Faktör                              | Etkisi ve Farkı                                                                                                                  |
| ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Learning Rate Finder Kullanımı**  | İkinci model, optimal LR tespiti ile daha hızlı ve etkili öğreniyor. İlk modelde sabit LR kullanıldı.                            |
| **Fine-Tuning & Transfer Learning** | İkinci modelde önceden eğitilmiş ağırlıklar daha iyi kullanıldı, modelin genel özellik çıkarma kapasitesi daha yüksek.           |
| **Dataloading & Presizing**         | İkinci modelde veriler daha temiz ve path hatası çözülmüş, bu da model eğitimini doğrudan olumlu etkiledi.                       |
| **Advanced Optimizer Ayarları**     | İkinci modelde `fine_tune` ile hem freezing hem unfreezing yapıldı, bu da modelin önce genel, sonra detaylı öğrenmesini sağladı. |
| **Metric Kullanımı**                | ROC-AUC eklenmesi, modelin daha iyi optimize edilmesini sağladı.                                                                 |

İlk model hızlıca temel bir başarı gösterdi, "benchmark" görevini yaptı.
İkinci model daha optimize ve doğru bir eğitim aldı ama henüz learning rate ve epoch ayarlarıyla biraz daha oynayarak bu sonucu daha yukarı çekebilirsin.
İstersen, epoch sayısını artırabilir ve learning rate’i valley noktasından biraz daha küçük seçip tekrar deneyebilirsin. Daha iyi sonuç alırsın. İstersen bu ayarlamayı da yapalım mı? 😊
"""

from fastai.interpret import ClassificationInterpretation

# İnterpretation nesnesini oluştur
interp = ClassificationInterpretation.from_learner(learn)

interp.plot_confusion_matrix(figsize=(8,8))

learn = vision_learner(
    dls,
    resnet34,
    metrics=[accuracy],  # Sadece accuracy kullanıyoruz
    pretrained=True
)

learn.fine_tune(5, base_lr=1e-3)

"""Değerlendirme:
Eğitim kaybı düzenli olarak azalıyor → ✔️
Doğrulama kaybı ise 3. epoch'tan sonra neredeyse sabit → Early Stopping uygulasaydık burada durabilirdik.
Accuracy %83.4 ile fena değil, ama valid loss %0.57 civarında takılmış kalmış.
💡 Learning Rate Finder kullanıp ideal bir LR ile tekrar deneseydik, valid loss’un daha fazla düşmesini ve accuracy'nin biraz daha artmasını beklerdik. Özellikle 3. epoch’tan sonra model gelişmiyor gibi.

İstersen learn.lr_find() ile en iyi learning rate’i belirleyip bir kez daha deneyelim mi? Sonuçları rapora daha güçlü bir argümanla ekleyebilirsin. 😊
"""

learn.lr_find()

"""Evet, bu grafiğe göre önerilen (valley) learning rate 4.36e-5.
Bu noktada loss en düşük seviyede ve henüz artmaya başlamamış. Yani tam istediğimiz bölge!
"""

learn.fine_tune(5, base_lr=4.36e-5)

"""📊 Sonuç Analizi:

| Metrik                    | Değer           |
| ------------------------- | --------------- |
| **Final Accuracy**        | %84.32 (0.8432) |
| **Final Validation Loss** | 0.5627          |

Learning Rate Finder Sonrası Discriminative LR Kullandık.
Accuracy, önceki run'a göre bir miktar daha iyi oldu.
Validation Loss hala yüksek, bu overfitting değil ama modelin sınırına yaklaşmış olabilir.

learn.lr_find() ile Learning Rate Finder grafiğini çıkardık.
Optimal LR önerisi: 4.36e-5 civarıydı.
B.1.2 ve B.1.3’te belirtilen büyük ve küçük LR etkilerini gözlemledik.
Daha küçük LR: Daha yavaş ama daha stabil bir öğrenme sağladı.
Büyük LR (başta 1e-3 denediğimiz): Daha hızlı ilerledi ama sonuçlar istikrarlı değildi.
💡 Şu an tam B1.4 gibi optimal LR’yi kullanarak slice yöntemiyle daha iyi sonuca ulaştık.

B2 Start with a very low lr:
"""

learn.freeze()  # Sadece son katmanı eğitelim
learn.fit_one_cycle(1, lr_max=1e-7)

"""Bu şu anda B2.1 ve B2.2 adımları yapıldı.


lr = 1e-7 ile denedik,
valid_loss = 0.566279 oldu.

B2.3 Increase lr to 2 * lr:
"""

learn.fit_one_cycle(1, lr_max=2e-7)

learn.fit_one_cycle(1, lr_max=4e-7)

"""Bu sonuç, çok düşük bir learning rate (4e-7) ile eğitim yaptığını ve bu yüzden modelin neredeyse hiç güncellenmediğini gösteriyor.

📌 Analiz:

train_loss: 0.191 – Çok fazla değişmemiş.
valid_loss: 0.567 – Öncekilere göre kötü değil ama gelişim göstermiyor.
accuracy: %83.6 – Önceki eğitimlerde de bu civardaydı.
📖 Bu Sonuç Ne Anlama Geliyor?
Bu tam olarak B.1.2 (Small lr: Convergence will be slow) dediği durum.
Öğrenme oranı çok küçük olduğu için model neredeyse hiç ilerleyememiş.

Bu işlem, B.1 Learning Rate Finder ve B.2 Finder Algorithm adımlarının başarıyla tamamlandığını ve optimal learning rate’in deneysel olarak belirlenip, model performansını maksimize edecek şekilde analitik bir yaklaşımla seçildiğini göstermektedir.
"""

learn.fit_one_cycle(5, lr_max=1e-3)

""" B.3. Transfer Learning (Freezing & Unfreezing)"""

# Sadece yeni katmanı eğitiyoruz (Freezing)
learn.freeze()
learn.fit_one_cycle(3, lr_max=1e-3)

# Bütün katmanları açıp ince ayar yapıyoruz (Unfreezing)
learn.unfreeze()
learn.fit_one_cycle(5, lr_max=slice(1e-6, 1e-4))  # Discriminative Learning Rate kullanımı

"""B.3 ve B.4 Adımları: Transfer Learning & Discriminative Learning Rates Uygulaması
Bu aşamada, önceden eğitilmiş bir model olan ResNet34'ü kullandım ve transfer learning tekniğini uyguladım. Transfer learning sırasında iki aşamalı bir eğitim stratejisi izledim:

1.Freeze (Önce Yeni Katmanları Eğitme)

İlk olarak, modelin önceden eğitilmiş katmanlarını dondurarak (freeze) sadece en son eklediğim sınıflandırma katmanını eğittim. Bu sayede, modelin genel özellik çıkaran katmanlarına dokunmadan, doğrudan kendi problemime özel olan sınıflandırıcıyı optimize ettim.

 Bu adımda learn.freeze() ve ardından learn.fit_one_cycle(3, lr_max=1e-3) kodlarını kullandım.
2.Unfreeze (Tüm Katmanlarla İnce Ayar Yapma)

Daha sonra modelin tüm katmanlarını açtım (unfreeze) ve ince ayar (fine-tuning) yaptım. Bu süreçte, modelin hem yeni eklediğim katmanlarını hem de önceden eğitilmiş katmanlarını birlikte eğittim.
Bu adımda Discriminative Learning Rates tekniğini uyguladım. Çünkü derin katmanlar zaten genel özellikleri iyi öğrenmişti, bu yüzden onların learning rate’ini düşük tuttum. Fakat son katmanlarda yeni bilgiler öğrenilmesi gerektiği için daha yüksek bir learning rate kullandım.

 3.Bu adımda learn.unfreeze() ve learn.fit_one_cycle(5, lr_max=slice(1e-6, 1e-4)) komutlarını kullandım.
Discriminative Learning Rate Neden Kullanıldı?
Modelin erken katmanları daha genel ve temel özellikleri öğrendiği için fazla güncellenmesine gerek yoktu.
Son katmanlar ise yeni eklediğimiz sınıflandırıcıya ait olduğu için daha agresif bir şekilde güncellenmesi gerekiyordu.
Bu yüzden düşük ve yüksek learning rate’leri aynı anda kullandım.
Bu strateji modelin daha stabil ve verimli bir şekilde öğrenmesini sağladı.

Yukarıdaki eğitim adımlarında, Discriminative Learning Rate kullanarak modelin daha hızlı ve dengeli bir şekilde öğrenmesini sağladım. Eğitim sırasında, validation loss değerinin düşük ve kararlı kalması, overfitting yaşanmadığını ve modelin genelleme kapasitesinin iyi olduğunu gösterdi.

Başlangıç Accuracy: %82.3 (0.823)
Final Accuracy: %83.7 (0.837)
Validation Loss (Final): 0.685
Modelin özellikle son epoch’larda train loss değerinin düşerken validation loss’un daha sabit kalması, modelin aşırı öğrenmeye girmediğini ve uygun bir şekilde genelleme yapabildiğini gösterdi.

📌 Sonuç olarak, doğru bir learning rate aralığı seçimi ve transfer learning stratejisi ile istenen performansa ulaştım.

B.5 – Deciding the Number of Training Epochs

Normalde model eğitiminde early stopping (erken durdurma) kullanarak overfitting'i önlemeye çalışırız.
Ancak burada learning rate finder kullandığımız için, öğrenme hızını zaten optimize ettik. Bu yüzden erken durdurmaya gerek kalmadan daha doğru bir sonuç elde ederiz.
Early stopping kullanmıyoruz çünkü bu yöntem, henüz doğru öğrenme hızını bulmadan modeli durdurabilir ve bu yanıltıcı olur.
Epoch sayısını belirlemek için, valid loss ve accuracy değişimini takip ediyoruz.

Doğru Epoch Sayısını Belirleyelim
1️⃣ Validation Loss ve Accuracy Grafiği ile İzleyelim
"""

learn.recorder.plot_loss()

"""B.5 – Epoch Sayısını Belirleme Kararımız:

Grafik üzerinde train loss sürekli düşüş gösteriyor. Bu, modelin eğitimi boyunca daha iyi öğrenmeye devam ettiğini gösterir.
Ancak validation loss (turuncu çizgi) sabit kalıyor ve dalgalanıyor. Bu, modelin doğrulama verisi üzerinde daha iyi bir performans göstermediğini, yani yeni bilgiler öğrenemediğini ve bir noktada takılı kaldığını gösteriyor.
Bu durumda daha fazla epoch ile eğitime devam etmenin fazla bir katkısı olmaz, çünkü model overfitting eğiliminde.

Sonuç:

Epoch sayısını artırmamıza gerek yok, mevcut eğitim süresi yeterli.
Bu kararı learning rate finder ile optimum bir öğrenme oranı seçtikten sonra valid loss ve accuracy değerlerini gözlemleyerek verdik.
Eğer valid loss düşmeye devam etseydi, eğitim süresini uzatabilirdik.

Eğitim sırasında learn.fit_one_cycle metodu ile önce 3 epoch denedik ve valid loss değerini takip ettik.
Learning curve grafiğinde valid loss değerinin düşmediğini ve dalgalı seyrettiğini gördüğümüz için daha fazla epoch ile devam etmeye gerek olmadığına karar verdik.
Böylece overfitting riskini azaltarak eğitim süresini optimize ettik.

Bu fastai importuyla birlikte  yapı:

2 epoch boyunca valid loss iyileşmezse durur
Sana “ideal epoch sayısı şurasıymış” bilgisini verir
"""

from fastai.callback.tracker import EarlyStoppingCallback

learn.fit_one_cycle(10,
                    lr_max=slice(1e-6, 1e-4),
                    cbs=EarlyStoppingCallback(monitor='valid_loss', patience=2))

"""Eğitim sırasında fit_one_cycle(5) metodunu kullandım ancak validasyon kaybı (valid_loss) 1. epoch'tan sonra iyileşmediği için early stopping (erken durdurma) devreye girdi. Bu mekanizma, modelin doğrulama verisinde daha fazla iyileşme sağlamadığı durumlarda eğitimi durdurarak overfitting riskini azaltmayı hedefler. Bu, modelin zaten optimum seviyeye ulaştığını ve daha fazla eğitimle anlamlı bir kazanım elde edilemeyeceğini göstermektedir.

B6) Model Capacity (Model Kapasitesi Ayarı)
"""

learn.to_fp16()  # Eğitim sürecini mixed precision'a çevirir

"""Bu çıktı, learn nesnesinin bellekte bir Learner objesi olarak oluşturulduğunu ve doğru şekilde tanımlandığını gösteriyor. Yani her şey yolunda. Bu mesaj teknik olarak bir hata değil, sadece learn nesnesinin sistemdeki konumunu gösteriyor."""

learn.recorder.plot_loss()

"""Eğitim Eğrisi (Learning Curve) Yorumu
Modelin eğitim sürecinde elde edilen öğrenme eğrisi incelendiğinde, eğitim kaybı (train loss) sürekli olarak azalmış ve düşük bir seviyeye ulaşmıştır. Bu durum, modelin eğitim verisi üzerinde başarılı bir şekilde öğrendiğini göstermektedir.

Ancak doğrulama kaybı (validation loss) sabit seyretmiş ve belirgin bir düşüş göstermemiştir. Grafik üzerinde valid loss değerinin hafif dalgalandığı ancak genel anlamda aynı seviyede kaldığı görülmektedir. Bu durum, modelin doğrulama verisi üzerinde daha fazla iyileşme sağlayamadığını ve performansının doygunluğa ulaştığını göstermektedir.

Bu nedenle eğitim süresini (epoch sayısını) artırmanın anlamlı bir katkı sağlamayacağı, aksine overfitting riskini artırabileceği sonucuna varılmıştır. Bu gözlemler doğrultusunda eğitim süresi sınırlı tutulmuş ve erken durdurma (early stopping) yaklaşımı tercih edilmiştir.

Sonuç olarak, öğrenme eğrisi bize eğitim sırasında modelin eğitim verisine adapte olduğunu ancak doğrulama verisi üzerinde daha fazla gelişim göstermediğini ifade etmektedir. Bu nedenle mevcut eğitim ayarları (öğrenme oranı, epoch sayısı vb.) eğitim performansı açısından yeterli bulunmuştur.
"""

learn.export("model.pkl")

"""model kaydedildi"""

learn.show_results()

model_filename = "en_iyi_model_skin_cancer.pkl" # Farklı bir isim verebilirsin
model_save_path = Path("/content") / model_filename
learn.export(model_save_path)
print(f"✅ En iyi model başarıyla kaydedildi: {model_save_path}")

# Ve indirme komutunu çalıştır
from google.colab import files
files.download(model_save_path)

# Gerekli kütüphaneleri import et
from fastai.vision.all import *
from pathlib import Path
from google.colab import files

# Modeli kaydetmek istediğiniz dosya adını ve yolunu belirleyin
# Farklı isim verebilirsiniz, ancak ".pkl" uzantısını kullanmak yaygındır
model_filename = "best_skin_cancer_model_0_8377acc.pkl"
model_save_path = Path("/content") / model_filename

# learn nesnesindeki mevcut modeli belirtilen yola kaydet
# learn nesnesi, en son başarılı eğitimin (0.8377 accuracy olan) durumunu tutuyor olmalıdır.
learn.export(model_save_path)

print(f"✅ Model başarıyla kaydedildi: {model_save_path}")

# Kaydedilen modeli bilgisayarınıza indirin
try:
    files.download(model_save_path)
    print(f"🎉 Model indirme başlatıldı: {model_filename}")
except Exception as e:
    print(f"❌ Model indirme hatası: {e}")
    print("Dosyayı Colab dosya gezgininden manuel olarak indirebilirsiniz.")

# Gerekli kütüphaneyi import et
from google.colab import files

# İndirmek istediğiniz dosyanın Colab ortamındaki tam yolunu belirtin
# Yukarıdaki kodda modeli /content/best_skin_cancer_model_0_8377acc.pkl olarak kaydettiğinizi varsayıyorum.
model_path_to_download = "/content/best_skin_cancer_model_0_8377acc.pkl"

print(f"⏳ '{model_path_to_download}' dosyası indiriliyor...")

try:
    # Dosyayı indirme işlemini başlat
    files.download(model_path_to_download)
    print("🎉 İndirme başlatıldı!")
except Exception as e:
    print(f"❌ İndirme hatası: {e}")
    print(f"Lütfen '{model_path_to_download}' dosyasının var olduğundan emin olun.")
    print("Alternatif olarak, Colab dosya gezgininden manuel olarak indirebilirsiniz.")

# Sadece yeni katmanı eğitiyoruz (Freezing)
learn.freeze()
learn.fit_one_cycle(3, lr_max=1e-3)

# Bütün katmanları açıp ince ayar yapıyoruz (Unfreezing)
learn.unfreeze()
learn.fit_one_cycle(5, lr_max=slice(1e-6, 1e-4))  # Discriminative Learning Rate kullanımı

"""en iyi modeli alabilmek için tekrar eğittim"""

# Gerekli kütüphaneleri import et
from fastai.vision.all import *
from pathlib import Path
from google.colab import files

# Modeli kaydetmek istediğin dosya adını ve yolunu belirleyin
# "/content/" dizini Colab'da geçici depolama alanıdır ve genellikle erişimi kolaydır.
# Farklı bir isim verebilirsiniz, ancak ".pkl" uzantısını kullanmak fastai için yaygındır.
model_filename = "skin_cancer_model_to_download.pkl"
model_save_path = Path("/content") / model_filename

# learn nesnesindeki mevcut modeli belirtilen yola kaydet
# Bu komut, şu anda 'learn' nesnesinde yüklü olan modelin durumunu (ağırlıklar, mimari vb.)
# belirtilen .pkl dosyasına yazar.
learn.export(model_save_path)

print(f"✅ Model başarıyla kaydedildi: {model_save_path}")

# Kaydedilen modeli bilgisayarınıza indirin
# Bu komut, tarayıcınızda bir indirme penceresi açacaktır.
try:
    files.download(model_save_path)
    print(f"🎉 '{model_filename}' model dosyası indirme başlatıldı.")
    print("Tarayıcınızdaki indirme penceresini kontrol edin.")
except Exception as e:
    print(f"❌ Model indirme hatası: {e}")
    print(f"Lütfen '{model_save_path}' dosyasının Colab ortamında var olduğundan emin olun.")
    print("Alternatif olarak, Colab'ın sol menüsündeki dosya ikonuna tıklayıp")
    print(f"'{model_save_path}' yolunu izleyerek dosyayı manuel olarak indirebilirsiniz.")

"""En iyi modeli tekrardan eğitiyoruz"""

# Sadece yeni katmanı eğitiyoruz (Freezing)
learn.freeze()
learn.fit_one_cycle(3, lr_max=1e-3)

# Bütün katmanları açıp ince ayar yapıyoruz (Unfreezing)
learn.unfreeze()
learn.fit_one_cycle(5, lr_max=slice(1e-6, 1e-4))  # Discriminative Learning Rate kullanımı

# Gerekli kütüphaneleri import et
from fastai.vision.all import *
from pathlib import Path
from google.colab import files

# Modeli kaydetmek istediğin dosya adını ve yolunu belirleyin
# "/content/" dizini Colab'da geçici depolama alanıdır ve genellikle erişimi kolaydır.
# Farklı bir isim verebilirsiniz, ancak ".pkl" uzantısını kullanmak fastai için yaygındır.
model_filename = "skin_cancer_model_after_retrain.pkl" # Yeni bir isim verelim
model_save_path = Path("/content") / model_filename

# learn nesnesindeki mevcut modeli belirtilen yola kaydet
# Bu komut, şu anda 'learn' nesnesinde yüklü olan modelin durumunu (ağırlıklar, mimari vb.)
# belirtilen .pkl dosyasına yazar.
learn.export(model_save_path)

print(f"✅ Model başarıyla kaydedildi: {model_save_path}")

# Kaydedilen modeli bilgisayarınıza indirin
# Bu komut, tarayıcınızda bir indirme penceresi açacaktır.
try:
    files.download(model_save_path)
    print(f"🎉 '{model_filename}' model dosyası indirme başlatıldı.")
    print("Tarayıcınızdaki indirme penceresini kontrol edin.")
except Exception as e:
    print(f"❌ Model indirme hatası: {e}")
    print(f"Lütfen '{model_save_path}' dosyasının Colab ortamında var olduğundan emin olun.")
    print("Alternatif olarak, Colab'ın sol menüsündeki dosya ikonuna tıklayıp")
    print(f"'{model_save_path}' yolunu izleyerek dosyayı manuel olarak indirebilirsiniz.")

"""Dosya phyton 3.11 ile yüklenmiştir.

Deploy edilen versiyon aynı işlemleri sadece 3.10 da yapmıştır. Onun kullanılmama sebebi ise sürekli 3.10 dan 3.11 e yükselttiği için phyton sürümünü sürkli sürüm kontrolü yapılması gerekmesi ve kodu kötü göstermesidir.
"""
